---
title: "Machine Learning"
author: "Akshay Goyal"
date: "13/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Importing libraries and setting working directory

```{r results='hold', message=FALSE, warning=FALSE}
library(dplyr) 
library(readr)
library(ggplot2)
library(pwr)
library(EnvStats)
library(reshape2)
library(PMCMR)
library(readxl)
```


### Exploring the data.

```{r results='hold', message=FALSE, warning=FALSE, echo=FALSE}
# read data
cars <- read_csv("cars.csv")

# factor the variables
cars$make <- factor(cars$make)
cars$fuel_type <- factor(cars$fuel_type, levels = c("gas", "diesel"))
cars$aspiration <- factor(cars$aspiration, levels = c("std", "turbo"))
cars$num_of_doors <- factor(cars$num_of_doors, levels = c("two", "four"))
cars$body_style <- factor(cars$body_style)
cars$drive_wheels <- factor(cars$drive_wheels)
cars$engine_location <- factor(cars$engine_location)
cars$engine_type <- factor(cars$engine_type)
cars$fuel_system <- factor(cars$fuel_system)

# Correlation among numerical variables
print("Correlation Matrix:")
Hmisc::rcorr(as.matrix(cars[,c(8:12,17:23)]))

# histogram of price
hist(cars$price, xlab = "Price", main = "Price Distribution")
hist(log(cars$price), xlab = "Log Price", main = "Ln Price Distribution")

# plot of categorical variables
boxplot(cars$price ~ cars$make, xlab = "Make", ylab = "Price", main = "Cars Make vs Price")
par(mfrow = c(2, 2))
boxplot(cars$price ~ cars$aspiration, xlab = "Aspiration",ylab = "Price", main = "Aspiration vs Price")
boxplot(cars$price ~ cars$fuel_type, xlab = "Fuel type",ylab = "Price", main = "Fuel type vs Price")
boxplot(cars$price ~ cars$body_style, xlab = "Body Style",ylab = "Price", main = "Body style vs Price")
boxplot(cars$price ~ cars$drive_wheels, xlab = "Drive wheel",ylab = "Price", main = "Drive wheel vs Price")


# plot of numerical variables
par(mfrow=c(1,1))
plot(cars$price~cars$horsepower, xlab = "horsepower", ylab = "price", main = "horsepower vs price")
par(mfrow = c(2, 2))
plot(cars$price~cars$wheel_base, xlab = "wheel base", ylab = "price", main = "wheel base vs price")
plot(cars$price~cars$curb_weight, xlab = "curb weight", ylab = "price", main = "curb weight vs price")
plot(cars$price~cars$length, xlab = "length", ylab = "price", main = "length vs price")
plot(cars$price~cars$city_mpg, xlab = "city mpg", ylab = "price", main = "city mpg vs price")
par(mfrow=c(1,1))

```

Findings :   

Categorical Variables:  
We see that there are primarily three segments of price targeted by different makes of the car. Some are premium, some deal in the middle range and some focus on lower price range cars.  
Turbo cars are generally more expensive.    
Hatchbacks are the lowest price cars and rear wheel drive are on the higher price range.   

Numerical Variables:  
Horsepower and city_mpg,highway_mpg are highly negatively correlated.   
Length width are positively correlated with length of the car.  

Price of the car increases as horsepower increases.  
Price of the car is also positively related to wheel base, length and weight of car.  
As city mpg of the car increases, price decreases as evident from the graph.  

### Building linear regression model using the initial set of variables.

```{r results='hold', message=FALSE}
#select data for initial linear model
data1 <- cars[ , c("price", "horsepower", "fuel_type", "aspiration", "num_of_doors",
                        "body_style", "drive_wheels", "make", "city_mpg", "length")]

# variables
price <- data1$price
hp <- data1$horsepower
ft <- data1$fuel_type
asp <- data1$aspiration
nod <- data1$num_of_doors
bs <- data1$body_style
dw <- data1$drive_wheels
make <- data1$make
cm <- data1$city_mpg
len <- data1$length

# plot relation between categorical fields and price
pairs(data1[,c(1,3:8)], panel = panel.smooth)

# plot relation between numerical fields and price
pairs(data1[,c(1,2,9,10)], panel = panel.smooth)

# As we can see from the scatter plots, relationship between price and horsepower is almost linear.
# Where as, relationship between price and city_mpg, length is not linear. So we need to apply some transformation to make relationship more linear.
# we will choose to apply log transformation to price.
# to maintain linear relationship between price and hp, we will apply log transformation to horsepower also.

# transforming the variables
# log and inverse transformations
ln_price <- log(price)
in_price <- 1/price
ln_cm <- log(cm)
ln_len <- log(len)
ln_hp <- log(hp)

# plot of the variables after transformation

pairs(matrix(c(ln_price,hp), nrow = 193, ncol = 2, dimnames = list(c(), c("ln_price", "horsepower"))), panel = panel.smooth)

pairs(matrix(c(ln_price,cm), nrow = 193, ncol = 2, dimnames = list(c(), c("ln_price", "city_mpg"))), panel = panel.smooth)

pairs(matrix(c(ln_price,len), nrow = 193, ncol = 2, dimnames = list(c(), c("ln_price", "length"))), panel = panel.smooth)

# the relationship curves are almost linear 
# so we will proceed with the transformed price variable
# also transformation accounts for correcting skewness in the price variable 

# checking for correlation
Hmisc::rcorr(as.matrix(data1[,c(1,2,9,10)])) 
# none of the independent variables seem to be having high correlation

# Initial model
rm <- lm(ln_price ~ hp + ft + asp + nod + bs + dw + make + cm + len)
summary(rm)
residuals_rm <- residuals(rm)

# normality assessment
nortest::ad.test(residuals(rm))
# normality assessment is satisfied for log price

car::vif(rm)
```

Final Model : 
The final model has an adjusted r-squared value of ~94% and is valid as per global f-test(p-value ~ 0).     
  1 unit increment in city mileage leads to decrease of 0.011% in price assuming all other variables constant.   
  Depending on the make of the car, price increase or decrease assuming all other variables constant.  
  1 unit increase in horsepower leads to increase of 0.004679% in price assuming all other variables constant.   
  1 unit increase in length leads to increase in price by 0.0109% assuming all other variables constant.  
  Depending on the body style of car, price may increase or decrease.  
  Turbo cars are on avg more expensive by 0.0298% assuming all other variables constant.   
  Depending on drive_wheel of the car, price may increase or decrease.  
  Cars with diesel fuel type are on avg more expensive by 0.232% assuming all other variables constant.   
  Cars with 4 doors are on avg more expensive by 0.077% assuming all other variables constant.     

### Improving the model     

1. We will first take all the variables in cars data set and develop a regression model, this will be our full model.  
2. We do Partial F-test to compare full model with our previous model from part 2 and see if the new variables add any significance.   
3. If the full model is significant, we will run Stepwise regression to come up with the best model using a p-value of 0.05.   
4. We will run regression diagnostics and check model validity and arrive at the final model.  

```{r results='hold', message=FALSE, warning=FALSE}

# add log price to main data set
cars$ln_price <- log(cars$price)

# make a subset
cars1 <- cars[,-24]

# full model
fm <- lm(ln_price ~ ., data = cars1)

# partial f-test
anova(rm, fm)

# step wise regression
k <- olsrr::ols_step_both_p(fm, prem = 0.05, pent = 0.05, details = TRUE)

# variables
cw <- cars$curb_weight
height <- cars$height
wb <- cars$wheel_base
el <- cars$engine_location


# final model
final_model <- lm(ln_price ~ cw + make + hp + bs + height + wb + asp + el)
summary(final_model)

residuals_final <- residuals(final_model)
residuals.final_sq <- residuals_final^2
predict_final <- predict(final_model)
predict_final_sq <- predict_final^2

# Multi Collinearity
print("Checking for Multi-Collinearity")
car::vif(final_model)

# Normality Assessment
hist(residuals_final, main = "Normality Assessment : Plot of Residuals") ## informal
nortest::ad.test(residuals(final_model)) ## formal

# Constant Variance Assessment
plot(predict_final, residuals_final, xlab = "Predict Y", ylab = "Residuals", main = "Constant Variance Assessment : Residuals vs Predict Y")

# Independence Assessment
### Graphical - Residuals(t) versus Residuals(t-1)
lag.plot(residuals_final, lags = 1, do.lines = FALSE, 
         diag = FALSE, 
         main = "Independence Assessment : Residuals versus Lag 1 Residuals")
abline(h=0, v=0)

```

Partial F-test:   
H_{0} : beta_{i} = 0    
H_{1} : At least one beta_{i} =/= 0      
We see the p-value in partial f-test is ~0 which means it is statistically significant. This implies that full model has newer variables which add significance to the inital model.   

Stepwise Regression:  
After stepwise regression using all the variables, we arrive at the following set of independent variables : curb_weight, make, horsepower, body_style, height, wheel_base, aspiration and engine_location.  

From business point of view it makes sense to include mileage also. But here horsepower is highly correlated to city and highway mileage, so that is accounted.    
Dimensions of the car also matter. Height is there in the variable set and length & width are highly correlated with curb_weight.    
Body style, aspiration and make - these are the attributes of car on which price is dependent.  

Regression diagnostics results:    
Multicollinearity : Not present as all the values of normalized gvif(last column in vif output) are less than 5.   
Normality - the residuals are normally distributed as seen from the plot of residuals and ad test.
Constant variance (homoscedasticity) : We see from the plot, that there are no patterns which implies this condition is satisfied.  
Independence : From this plot also we see no patterns, which implies this condition is satisfied.   

Final Model : 
The final model has an adjusted r-squared value of ~95% and is valid as per global f-test(p-value ~ 0).     
  1 unit increment in curb weight leads to increase of 0.00054% in price assuming all other variables constant.   
  Depending on the make of the car, price increase or decrease assuming all other variables constant.  
  1 unit increase in horsepower leads to increase of 0.001956% in price assuming all other variables constant.   
  1 unit increase in height leads to decrease in price by 0.03008% assuming all other variables constant.  
  1 unit increase in wheel base leads to increase of 0.01573% in price assuming all other variables constant.  
  Depending on the body syle of car, price may increase or decrease.  
  Turbo cars are on avg more expensive by 0.078% assuming all other variables constant.   
  Cars with engine at the rear are on avg more expensive by 0.349% assuming all other variables constant.
  

### Controlling the FDR in variables with a q of 0.1 using BH procedure   

```{r results='hold', message=FALSE, warnings = FALSE}
# get p-values
p_value_table <- as.data.frame(summary(final_model)$coefficients[,4])
p_value_table$variables <- rownames(p_value_table)
rownames(p_value_table) <- NULL
colnames(p_value_table) <- c('p_value', 'variables')

# rank p-values
p_value_table <- p_value_table %>% mutate(rank_p = rank(p_value, ties.method = 'min'))

# BH metric
p_value_table$bh <- 0.1 * p_value_table$rank_p / nrow(p_value_table)

# Checking if p-value is less than BH metric
p_value_table$label <- ifelse(p_value_table$p_value <= p_value_table$bh, TRUE, FALSE)
p_value_table[,c(2,1,3,5)] %>% arrange(rank_p)

# Taking the subset of p-values
subset_p <- p_value_table[p_value_table$label == TRUE,]

## FDR function - extract p-value cutoff for E[fdf] < q
fdr <- function(pvals, q, plotit=FALSE){
  pvals <- pvals[!is.na(pvals)]
  N <- length(pvals)
  
  k <- rank(pvals, ties.method="min")
  alpha <- max(pvals[ pvals <= (q*k/N) ])
  
  if(plotit){
    sig <- factor(pvals <= alpha)
    o <- order(pvals)
    plot(pvals[o], log="xy", col=c("grey60","red")[sig[o]], pch=20, 
      ylab="p-values", xlab="tests ordered by p-value", main = paste('FDR =',q))
    lines(1:N, q*(1:N) / N)
  }
  
  return(alpha)
}

# BHvalue for cutoff and plot
fdr(p_value_table$p_value, 0.1, plotit = TRUE)
```

In our improved model, we want to minimize FDR. We started with q of 0.1 and used Benjamini-Hochberg procedure to check p-values.   
We got a cut-off of 0.06734374 for p-values, which implies to keep the fdr <= 0.1, we should consider variables with p-value less than or equal to 0.06734374.   
Using this cut-off value, we were able to reduce the number of variables from 31 to 22. We can see from the table, some of the "make..." variables are above the p-value cut-off and are in the rejection region.    
Now our fdr is less than or equal to 0.1, which means number of false positives is less than or equal to 2.2 in 22 variables. This implies we should have 20 true discoveries in our model.  